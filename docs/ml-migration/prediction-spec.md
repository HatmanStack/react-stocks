# Python Logistic Regression Prediction Specification

## Overview

This document specifies the behavior of the Python scikit-learn logistic regression prediction service for stock price predictions. This specification serves as the reference for implementing an identical JavaScript version.

**Python Service Repository**: https://github.com/HatmanStack/python-logistic-prediction
**Deployed Endpoint**: https://stocks-f3jmjyxrpq-uc.a.run.app/predict
**Model**: scikit-learn LogisticRegressionCV with 8-fold cross-validation
**Purpose**: Binary classification (price up=0 or down=1) for 3 time horizons

## Service Interface

### Input Format

```json
{
  "ticker": "AAPL",
  "close": [150.0, 152.0, 151.5, ...],
  "volume": [100000000, 95000000, 98000000, ...],
  "positive": [5, 3, 7, ...],
  "negative": [2, 4, 1, ...],
  "sentiment": ["POS", "NEG", "NEUT", "POS", ...]
}
```

**Field Descriptions**:
- `ticker`: Stock symbol (string)
- `close`: Array of closing prices (number[])
- `volume`: Array of trading volumes (number[])
- `positive`: Array of positive word counts from sentiment analysis (number[])
- `negative`: Array of negative word counts from sentiment analysis (number[])
- `sentiment`: Array of sentiment categories (string[] - "POS", "NEG", "NEUT", or "UNKNOWN")

**Requirements**:
- All arrays must be same length
- Minimum length: 29 points (8 folds + 21 day horizon for month prediction)
- Values must be valid numbers (no NaN, Infinity)

### Output Format

```json
{
  "next": "0.0",
  "week": "1.0",
  "month": "0.0",
  "ticker": "AAPL"
}
```

**Field Descriptions**:
- `next`: Next day prediction (0=up, 1=down) as string
- `week`: 2-week prediction (0=up, 1=down) as string
- `month`: 1-month prediction (0=up, 1=down) as string
- `ticker`: Echo of input ticker

**Note**: Predictions are returned as string representations of floats (e.g., "0.0", "1.0")

## Feature Engineering

### Feature Matrix Construction

The model uses 8 features per observation:

1. **close** (numerical)
2. **volume** (numerical)
3. **positive** (numerical) - positive word count
4. **negative** (numerical) - negative word count
5. **is_pos** (binary) - 1 if sentiment="POS", else 0
6. **is_neg** (binary) - 1 if sentiment="NEG", else 0
7. **is_neut** (binary) - 1 if sentiment="NEUT", else 0
8. **is_unknown** (binary) - 1 if sentiment not in [POS, NEG, NEUT], else 0

**One-Hot Encoding Logic**:
```python
def encode_sentiment(sentiment_value):
    if sentiment_value == "POS":
        return [1, 0, 0, 0]
    elif sentiment_value == "NEG":
        return [0, 1, 0, 0]
    elif sentiment_value == "NEUT":
        return [0, 0, 1, 0]
    else:  # UNKNOWN or any other value
        return [0, 0, 0, 1]
```

### Label Generation

For each time horizon, labels are generated by comparing current price to future price:

```python
def create_labels(close_prices, horizon):
    labels = []
    for i in range(len(close_prices) - horizon):
        if close_prices[i] > close_prices[i + horizon]:
            labels.append(1)  # Price will drop
        else:
            labels.append(0)  # Price will rise or stay same
    return labels
```

**Horizons**:
- Next day: horizon = 1
- 2 weeks: horizon = 10 (trading days)
- 1 month: horizon = 21 (trading days)

**Important**: The last `horizon` data points cannot be labeled (no future data).

## StandardScaler

### Algorithm

scikit-learn's StandardScaler performs z-score normalization:

```python
# Fit (calculate mean and std)
mean = sum(X_column) / n
std = sqrt(sum((X_column - mean)^2) / n)  # Population std, NOT sample

# Transform
X_scaled = (X - mean) / std
```

**Critical**: Uses **population standard deviation** (divide by n), not sample std (divide by n-1).

### Formula Details

For each feature column:

```
mean = Σ(x_i) / n
variance = Σ((x_i - mean)^2) / n
std = √variance
scaled_value = (x - mean) / std
```

### Edge Cases

- **Constant feature** (std=0): Scaled to 0.0
- **Single data point**: mean=x, std=0, result=0.0
- **NaN values**: Should throw error (not handled)

### Inverse Transform

```python
X_original = X_scaled * std + mean
```

### Verification

After transformation:
- Mean of scaled features ≈ 0.0 (within floating point precision)
- Std of scaled features ≈ 1.0 (within floating point precision)

## Logistic Regression Model

### Model Configuration

**Algorithm**: Logistic Regression with L2 regularization
**Solver**: liblinear (default for small datasets)
**Regularization**: C=1.0 (inverse regularization strength)
**Max Iterations**: 100 (default)
**Class Weight**: None (balanced classes)

### Training Process

1. Split data into features (X) and labels (y)
2. Apply StandardScaler to X (fit on training data)
3. Train LogisticRegression model on scaled X and y
4. Store trained model and scaler

### Prediction Process

1. Take most recent observation (last row of feature matrix)
2. Scale using fitted StandardScaler
3. Predict using trained LogisticRegression model
4. Return binary prediction (0 or 1)

### Mathematical Model

Logistic regression predicts probability using sigmoid function:

```
z = w_0 + w_1*x_1 + w_2*x_2 + ... + w_n*x_n
P(y=1|x) = 1 / (1 + e^(-z))
prediction = 1 if P(y=1|x) >= 0.5 else 0
```

Where:
- w_0 = bias (intercept)
- w_1...w_n = feature weights (coefficients)
- x_1...x_n = scaled feature values

## Cross-Validation

### Configuration

**Method**: K-Fold Cross-Validation
**K**: 8 folds
**Stratification**: None (sequential splits)
**Shuffle**: False (maintains temporal order)

### K-Fold Split Logic

Data is divided into 8 sequential parts:

```python
fold_size = len(X) // k
for i in range(k):
    test_start = i * fold_size
    test_end = (i + 1) * fold_size if i < k-1 else len(X)
    test_indices = range(test_start, test_end)
    train_indices = [j for j in range(len(X)) if j not in test_indices]
```

### Training Process

For each fold:
1. Split data into train (7/8) and test (1/8)
2. Fit StandardScaler on training data
3. Transform both train and test data
4. Train LogisticRegression on training data
5. Evaluate on test data
6. Record accuracy score

After all folds:
1. Calculate mean CV score (diagnostic only)
2. Train final model on ALL data
3. Use final model for predictions

**Note**: The final model uses all available data, not just the last fold.

## Numerical Precision Requirements

### Tolerance Levels

**StandardScaler**:
- Mean/std calculation: 6 decimal places (0.000001)
- Scaled features: 6 decimal places (0.000001)

**Predictions**:
- Final predictions: 4 decimal places (0.0001)
- Probabilities: 4 decimal places (0.0001)

### Floating Point Considerations

- JavaScript uses IEEE 754 double precision (same as Python)
- Both have 15-17 significant decimal digits
- Cumulative rounding errors possible in matrix operations
- Use consistent rounding: `Math.round(x * 10000) / 10000` for 4 decimals

### Known Precision Issues

1. **Matrix multiplication**: Small differences can accumulate
2. **Exponential function**: `exp()` can overflow for large negative/positive values
3. **Summation**: Order of operations affects precision
4. **Division by zero**: Check for zero std before scaling

## Reference Data

### Sample Inputs

**AAPL** (30 data points):
- Close: [150.0, 152.0, 151.5, 153.0, ...]
- Volume: [100000000, 95000000, 98000000, ...]
- Positive: [5, 3, 7, 4, ...]
- Negative: [2, 4, 1, 3, ...]
- Sentiment: ["POS", "NEG", "NEUT", "POS", ...]

### Expected StandardScaler Parameters

Example for close prices [150.0, 152.0, 151.5]:
- Mean: 151.1667
- Std: 0.8498 (population std)

### Expected Predictions

See `prediction-samples.json` for complete test cases.

## Implementation Notes

### Critical Implementation Details

1. **Population vs Sample Std**: MUST use population (÷n), not sample (÷(n-1))
2. **One-hot encoding order**: [is_pos, is_neg, is_neut, is_unknown]
3. **Label logic**: 1 if price drops, 0 if price rises
4. **Final prediction**: Use most recent observation AFTER training on all historical data
5. **String output**: Convert predictions to strings (e.g., "0.0", "1.0")

### Common Pitfalls

- Using sample std instead of population std → Wrong scaled values
- Incorrect one-hot encoding order → Model uses wrong features
- Training on scaled data but predicting on unscaled → Wrong results
- Predicting on training data instead of most recent → Leakage
- Returning numbers instead of strings → Type mismatch

## Testing Strategy

### Component Tests

1. **StandardScaler**: Test with known inputs, verify mean=0, std=1 after transform
2. **One-hot encoding**: Test all sentiment categories
3. **Label generation**: Test with known price sequences
4. **Logistic regression**: Test on linearly separable data

### Integration Tests

1. **Full pipeline**: Input → Features → Scaling → Training → Prediction
2. **Numerical precision**: Compare to Python outputs (4 decimal tolerance)
3. **Edge cases**: Insufficient data, constant features, all same labels

### Performance Tests

1. **Latency**: <50ms for 30 data points on web
2. **Memory**: No memory leaks on repeated calls
3. **Accuracy**: Predictions match Python service

## Validation Checklist

- [ ] StandardScaler produces mean≈0, std≈1
- [ ] Scaled features match Python to 6 decimals
- [ ] One-hot encoding produces correct 8-feature matrix
- [ ] Labels generated correctly for all horizons
- [ ] Predictions match Python service to 4 decimals
- [ ] Performance <50ms on web platform
- [ ] All edge cases handled (insufficient data, NaN, etc.)

## References

- **scikit-learn StandardScaler**: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
- **scikit-learn LogisticRegressionCV**: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html
- **Python Service Repo**: https://github.com/HatmanStack/python-logistic-prediction
